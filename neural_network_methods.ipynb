{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import Package \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import time\n",
    "import copy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "\"\"\" Global variables \"\"\"\n",
    "DATA = \"./data\"\n",
    "TRAIN_DATA_PATH = f\"{DATA}/training.csv\"\n",
    "TEST_DATA_PATH = f\"{DATA}/test.csv\"\n",
    "OUTPUT_PREDICTION = f\"{DATA}/neural_network_predictions.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to find the relationthips among the features in the dataset in order to help us decide how to utilize the combination of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load dataset \"\"\"\n",
    "class SearchEngineDataLoader(Dataset):\n",
    "    def __init__(self, file_path: str, train: bool, features: list):\n",
    "        self.data = pd.read_csv(file_path)\n",
    "        self.features = self.data[features].values\n",
    "        if train:\n",
    "            self.labels = self.data['relevance'].values\n",
    "        else:\n",
    "            self.labels = None\n",
    "        \n",
    "        self.features = self.data[features].values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = torch.tensor(self.features[idx], dtype=torch.float32)\n",
    "        if self.labels is not None:\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32).unsqueeze(0)\n",
    "            return features, label\n",
    "        else:\n",
    "            return features\n",
    "\n",
    "\n",
    "def get_dataset(train_file_path: str,\n",
    "                test_file_path: str,\n",
    "                features: list):\n",
    "    \"\"\" Create Dataset \"\"\"\n",
    "    train_dataset = SearchEngineDataLoader(train_file_path,\n",
    "                                           train=True,\n",
    "                                           features=features)\n",
    "    test_dataset = SearchEngineDataLoader(test_file_path,\n",
    "                                          train=False,\n",
    "                                          features=features)\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def get_dataloader(train_dataset: SearchEngineDataLoader,\n",
    "                   test_dataset: SearchEngineDataLoader,\n",
    "                   batch_size: int, \n",
    "                   val_proportion: float = 0,\n",
    "                   pin_memory: bool = True,\n",
    "                   shuffle: bool = True,\n",
    "                   seed: int = 1) -> tuple[DataLoader, DataLoader, int, int]:\n",
    "    \"\"\" Create Dataloader and return in_channels and num_classes \"\"\"\n",
    "    in_channels = train_dataset.features.shape[1]\n",
    "    num_classes = 1  # Assuming binary classification\n",
    "\n",
    "    # Split into train and validation sets if val_proportion > 0\n",
    "    if val_proportion > 0:\n",
    "        val_size = int(len(train_dataset) * val_proportion)\n",
    "        train_size = len(train_dataset) - val_size\n",
    "        generator = torch.Generator().manual_seed(seed)\n",
    "        train_dataset, val_dataset = random_split(train_dataset, \n",
    "                                                  [train_size, val_size], \n",
    "                                                  generator=generator)\n",
    "    else:\n",
    "        train_dataset = train_dataset\n",
    "        val_dataset = None\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, \n",
    "                              batch_size=batch_size, \n",
    "                              shuffle=shuffle, \n",
    "                              num_workers=0, \n",
    "                              pin_memory=pin_memory)\n",
    "    \n",
    "    if val_dataset is not None:\n",
    "        val_loader = DataLoader(val_dataset, \n",
    "                                batch_size=batch_size, \n",
    "                                shuffle=False, \n",
    "                                num_workers=0, \n",
    "                                pin_memory=pin_memory)\n",
    "    else:\n",
    "        val_loader = None\n",
    "    \n",
    "    test_loader = DataLoader(test_dataset, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False, \n",
    "                             num_workers=0, \n",
    "                             pin_memory=pin_memory)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, in_channels, num_classes\n",
    "\n",
    "\n",
    "features = ['query_length', 'is_homepage', 'sig1', 'sig2', \n",
    "            'sig3', 'sig4', 'sig6', 'sig7', 'sig8']\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset, test_dataset = get_dataset(TRAIN_DATA_PATH,\n",
    "                                          TEST_DATA_PATH,\n",
    "                                          features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize both dataset \n",
    "scaler = StandardScaler()\n",
    "train_dataset.features = scaler.fit_transform(train_dataset.features)\n",
    "test_dataset.features = scaler.transform(test_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Get Dataloader \"\"\"\n",
    "train_loader, val_loader, test_loader, in_channels, num_class = get_dataloader(train_dataset=train_dataset,\n",
    "                                                                               test_dataset=test_dataset,\n",
    "                                                                               batch_size=batch_size,\n",
    "                                                                               val_proportion=0.2,\n",
    "                                                                               pin_memory=True,\n",
    "                                                                               shuffle=True,\n",
    "                                                                               seed=1,)\n",
    "print(f\"Sample numbers: {train_dataset.features.shape[0]}\")\n",
    "print(f\"Feature numbers: {train_dataset.features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modeling using Neural Network \"\"\"\n",
    "def torch_set_random_seed(seed: int = 1) -> None:\n",
    "    \"\"\" Set random seed for reproducible usage \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "def train(model: nn.Module, \n",
    "          train_loader: DataLoader, \n",
    "          loss_function: nn.Module,\n",
    "          optimizer: optim.Optimizer,\n",
    "          epoch: int,\n",
    "          device: str) -> float:\n",
    "    \"\"\" Train model and save using early stop on test dataset \"\"\"\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for features, labels in train_loader:\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module,\n",
    "             eval_loader: DataLoader, \n",
    "             loss_function: nn.Module, \n",
    "             threshold: float,\n",
    "             device: str) -> tuple[float, float]:\n",
    "    \"\"\" Evaluate model \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in eval_loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(features)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > threshold).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    average_loss = total_loss / len(eval_loader)\n",
    "    return accuracy, average_loss\n",
    "\n",
    "\n",
    "\"\"\" Train model on dataloader \"\"\"\n",
    "min_lr = 1e-4\n",
    "random_seed = 1\n",
    "split_seed = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Running on device: {device}\")\n",
    "\n",
    "output_dir = \"output_models\"\n",
    "overall_best_acc = 0\n",
    "optimal_weight_decay = 0\n",
    "optimal_initial_lr = 0\n",
    "optimal_total_epoch = 0\n",
    "\n",
    "for weight_decay in [1e-2, 1e-3, 1e-4]:\n",
    "    for initial_lr in [1e-1, 5e-2, 1e-2]:\n",
    "        for total_epoch in [10, 20]:\n",
    "            best_acc = 0.0\n",
    "            for split_seed in [1]:\n",
    "                torch_set_random_seed(random_seed)\n",
    "                hyperparams_config = {\n",
    "                    \"epoch\": total_epoch,\n",
    "                    \"initial_lr\": initial_lr,\n",
    "                    \"min_lr\": min_lr,\n",
    "                    \"random_seed\": random_seed\n",
    "                }\n",
    "                wandb.init(\n",
    "                    project=f\"STATS-Project-resnet18-final\",\n",
    "                    name=f\"{int(time.time())}\",\n",
    "                    id=str(int(time.time())),\n",
    "                    config=hyperparams_config,\n",
    "                    mode='online'\n",
    "                )\n",
    "\n",
    "                from models.resnet import resnet18, resnet34, resnet50, resnet101, resnet152\n",
    "                from models.vgg import vgg11, vgg13, vgg16, vgg19\n",
    "                model = resnet50(in_channels, num_class).to(device)\n",
    "\n",
    "                loss_function = nn.BCEWithLogitsLoss()\n",
    "                optimizer = optim.SGD(model.parameters(), \n",
    "                                      lr=initial_lr, \n",
    "                                      momentum=0.9, \n",
    "                                      weight_decay=weight_decay)\n",
    "                lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
    "                                                                    T_max=total_epoch, \n",
    "                                                                    eta_min= min_lr,\n",
    "                                                                    last_epoch=-1)\n",
    "\n",
    "                cur_best_acc = 0\n",
    "                with tqdm(total=total_epoch, desc=f'Training', unit='epoch') as pbar:\n",
    "                    for epoch in range(1, total_epoch + 1):\n",
    "                        train_loss = train(model, \n",
    "                                            train_loader, \n",
    "                                            loss_function, \n",
    "                                            optimizer,\n",
    "                                            epoch,\n",
    "                                            device)\n",
    "                        top1_acc, eval_loss = evaluate(model, \n",
    "                                                    val_loader, \n",
    "                                                    loss_function, \n",
    "                                                    0.5,\n",
    "                                                    device)\n",
    "                        lr_scheduler.step()\n",
    "\n",
    "                        if cur_best_acc < top1_acc:\n",
    "                            cur_best_acc = top1_acc\n",
    "                            if best_acc < cur_best_acc:\n",
    "                                best_acc = cur_best_acc\n",
    "                                os.makedirs(f\"{output_dir}\", exist_ok=True)\n",
    "                                output_pth = f\"{output_dir}/best_model.pth\"\n",
    "                                torch.save(model, output_pth)\n",
    "                        \n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            lr = param_group['lr']\n",
    "                        \n",
    "                        wandb.log({\"epoch\": epoch, \"train_loss\": train_loss, 'lr': lr,\n",
    "                                   \"top1_acc\": top1_acc, \"eval_loss\": eval_loss,\n",
    "                                   \"best top1 acc\": cur_best_acc})\n",
    "                        \n",
    "                        pbar.set_postfix({'Train loss': train_loss, \n",
    "                                          'overall_best_acc': overall_best_acc, \n",
    "                                          'Best top1 acc': cur_best_acc, \n",
    "                                          'Top1 acc': top1_acc})\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                wandb.finish()\n",
    "                print(f\"Seed {split_seed} has best_acc: {cur_best_acc}\")\n",
    "            if overall_best_acc <= best_acc:\n",
    "                overall_best_acc = best_acc\n",
    "                optimal_weight_decay = weight_decay\n",
    "                optimal_initial_lr = initial_lr\n",
    "                optimal_total_epoch = total_epoch\n",
    "\n",
    "print(f\"Optimal validation accuracy: {overall_best_acc}\")\n",
    "print(f\"Optimal weight decayL: {optimal_weight_decay}\")\n",
    "print(f\"Optimal initial learning rate: {optimal_initial_lr}\")\n",
    "print(f\"Optimal total epoch: {optimal_total_epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Predict on test dataset using the best model \"\"\"\n",
    "@torch.no_grad()\n",
    "def predict_and_save_results(model, test_loader, output_file, threshold):\n",
    "    model.eval() \n",
    "    results = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, features in enumerate(test_loader):\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "            predictions = (torch.sigmoid(outputs) > threshold).float()\n",
    "\n",
    "            for idx2, prediction in enumerate(predictions):\n",
    "                sample_id = str(int(test_loader.dataset.data.iloc[idx*batch_size+idx2]['query_id'])) + str(int(test_loader.dataset.data.iloc[idx*batch_size+idx2]['url_id']))\n",
    "                results.append({'id': sample_id, 'relevance': int(prediction)})\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "best_model = torch.load(f\"{output_dir}/best_model.pth\").to(device)\n",
    "predict_and_save_results(best_model, test_loader, OUTPUT_PREDICTION, 0.5)\n",
    "print(f\"Predictions saved at {OUTPUT_PREDICTION}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
