{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Import Package \"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import SplineTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\"\"\" Global variables \"\"\"\n",
    "DATA = \"./data\"\n",
    "TRAIN_DATA_PATH = f\"{DATA}/training.csv\"\n",
    "TEST_DATA_PATH = f\"{DATA}/test.csv\"\n",
    "OUTPUT_PREDICTION = f\"{DATA}/traditional_method_predictions.csv\"\n",
    "REMOVE_OUTLIER = True\n",
    "STANDARDIZE_DATA = False\n",
    "APPLY_KERNEL_METHOD = False\n",
    "FEATURE_SELECTION = \"Forward_stepwise_selection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load dataset \"\"\"\n",
    "train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "used_features = ['query_length', 'is_homepage', 'sig1', 'sig2', \n",
    "                 'sig3', 'sig4', 'sig5', 'sig6', 'sig7', 'sig8']\n",
    "unused_features = ['query_id', 'url_id', 'relevance', 'id']\n",
    "detect_features = ['sig1', 'sig2', 'sig3', \n",
    "                   'sig5', 'sig6', 'sig7', 'sig8']\n",
    "label = 'relevance'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Remove outlier from training dataset \"\"\"\n",
    "if REMOVE_OUTLIER:\n",
    "    original_sample_num = train_data.shape[0]\n",
    "\n",
    "    mask = pd.Series([True] * train_data.shape[0], index=train_data.index)\n",
    "\n",
    "    for feature in detect_features:\n",
    "        if feature in train_data:\n",
    "            Q1 = train_data[feature].quantile(0.25)\n",
    "            Q3 = train_data[feature].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            feature_mask = (train_data[feature] >= lower_bound) & (train_data[feature] <= upper_bound)\n",
    "            outliers_num = (~feature_mask).sum()\n",
    "\n",
    "            print(feature_mask.sum())\n",
    "            print(f\"Feature '{feature}': {outliers_num} outliers\")\n",
    "\n",
    "            mask &= feature_mask\n",
    "\n",
    "    train_data = train_data[mask]\n",
    "\n",
    "    washed_sample_num = train_data.shape[0]\n",
    "    print(f\"Removed {original_sample_num - washed_sample_num} outliers from dataset\")\n",
    "    print(f\"Remaining samples: {washed_sample_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Standardize both dataset \"\"\"\n",
    "if STANDARDIZE_DATA:\n",
    "    scaler = StandardScaler()\n",
    "    train_data[used_features] = scaler.fit_transform(train_data[used_features])\n",
    "    test_data[used_features] = scaler.transform(test_data[used_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Apply kernel methods to increase available features without dropping original features \"\"\"\n",
    "if APPLY_KERNEL_METHOD:\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "    train_poly = pd.DataFrame(poly.fit_transform(train_data[used_features]), index=train_data.index)\n",
    "    test_poly = pd.DataFrame(poly.transform(test_data[used_features]), index=test_data.index)\n",
    "\n",
    "    train_poly.columns = [f\"poly_{i}\" for i in range(train_poly.shape[1])]\n",
    "    test_poly.columns = [f\"poly_{i}\" for i in range(test_poly.shape[1])]\n",
    "\n",
    "    original_features = train_data[used_features].shape[1]\n",
    "    extended_features = train_poly.shape[1]\n",
    "\n",
    "    train_data = pd.concat([train_data, train_poly], axis=1)\n",
    "    test_data = pd.concat([test_data, test_poly], axis=1)\n",
    "\n",
    "    print(f\"Added {extended_features - original_features} features\")\n",
    "    print(f\"Total features after expansion: {train_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Apply forward stepwise feature selection \"\"\"\n",
    "if FEATURE_SELECTION == \"Forward_stepwise_selection\":\n",
    "    all_features = list(train_data.columns)\n",
    "    print(all_features)\n",
    "    for unused_feature in unused_features:\n",
    "        all_features.remove(unused_feature)\n",
    "    print(all_features)\n",
    "\n",
    "    optimal_features = []\n",
    "    optimal_cur_features = []\n",
    "    optimal_pre_features = []\n",
    "    best_acc = 0\n",
    "    for idx in range(len(all_features)):\n",
    "        print(f\"Starting Step {idx} ...\")\n",
    "        best_cur_acc = 0\n",
    "        for new_feature in all_features:\n",
    "            cur_features = optimal_pre_features[:]\n",
    "            if new_feature not in optimal_pre_features:\n",
    "                cur_features.append(new_feature)\n",
    "            else:\n",
    "                continue\n",
    "            random.seed(1)\n",
    "            np.random.seed(1)\n",
    "\n",
    "            cur_train_features = train_data[cur_features]\n",
    "            cur_train_label = train_data[label]\n",
    "            \n",
    "            spline_transformer = SplineTransformer(n_knots=100, degree=3)\n",
    "            model = LogisticRegression(random_state=1)\n",
    "            pipeline = make_pipeline(spline_transformer, model)\n",
    "\n",
    "            kf = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "            cv_scores = cross_val_score(pipeline, cur_train_features, cur_train_label, \n",
    "                                        cv=kf, scoring='accuracy')\n",
    "            \n",
    "            if best_cur_acc <= np.mean(cv_scores):\n",
    "                optimal_cur_features = cur_features\n",
    "                best_cur_acc = np.mean(cv_scores)\n",
    "                print(f\"Update acc: {best_cur_acc} with features: {optimal_cur_features}\")\n",
    "\n",
    "        optimal_pre_features = optimal_cur_features\n",
    "        if best_acc <= best_cur_acc:\n",
    "            optimal_features = optimal_cur_features\n",
    "            best_acc = best_cur_acc\n",
    "    print(f\"Optimal features combination: {optimal_features} with best acc: {best_acc}\")\n",
    "    train_features = train_data[optimal_features]\n",
    "    train_label = train_data[label]\n",
    "    test_features = test_data[optimal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Apply PCA to remove unnecessary features \"\"\"\n",
    "if FEATURE_SELECTION == \"PCA\":\n",
    "    pca = PCA()\n",
    "    pca.fit(train_data[used_features])\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_\n",
    "    cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', linestyle='--')\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('Cumulative Explained Variance Ratio by PCA')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= 0.999) + 1\n",
    "    print(f\"Number of principal components to retain (99% variance): {n_components}\")\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    train_features = pca.fit_transform(train_data[used_features])\n",
    "    train_label = train_data[label]\n",
    "    test_features = pca.transform(test_data[used_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Visualize the relationships among features \"\"\"\n",
    "sns.pairplot(pd.DataFrame(train_features))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modeling using logistic regression  \"\"\"\n",
    "model = LogisticRegression(\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'penalty': ['l2'], \n",
    "        'C': [0.1, 1, 10], \n",
    "        'solver': ['newton-cg', 'lbfgs', 'sag'], \n",
    "        'max_iter': [500]\n",
    "    },\n",
    "    {\n",
    "        'penalty': ['l1'], \n",
    "        'C': [0.1, 1, 10], \n",
    "        'solver': ['liblinear'], \n",
    "        'max_iter': [500]\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=1), \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(train_features, train_label)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(train_features, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modeling using RandomForest Classifier  \"\"\"\n",
    "model = RandomForestClassifier(\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_split': [2, 4],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features': ['sqrt'],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=1), \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(train_features, train_label)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(train_features, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Modeling using Light Gradient Boosting  \"\"\"\n",
    "model = lgb.LGBMClassifier(\n",
    "    random_state=1,\n",
    "    verbosity=-1,\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [20, 30, 40],\n",
    "    'max_depth': [-1],\n",
    "    'learning_rate': [0.1, 1],\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'min_child_samples': [20],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8],\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=1), \n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(train_features, train_label)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation accuracy: \", grid_search.best_score_)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "best_model.fit(train_features, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Predict on test dataset using the best model \"\"\"\n",
    "results = []\n",
    "predictions = best_model.predict(test_features)\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "for idx, prediction in enumerate(predictions):\n",
    "    sample_id = str(int(test_data.iloc[idx]['query_id'])) + str(int(test_data.iloc[idx]['url_id']))\n",
    "    results.append({'id': sample_id, 'relevance': int(prediction)})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(OUTPUT_PREDICTION, index=False)\n",
    "print(f\"Predictions saved at {OUTPUT_PREDICTION}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
